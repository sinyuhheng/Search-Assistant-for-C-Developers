{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HkB1oYjZ0rJn",
   "metadata": {
    "id": "HkB1oYjZ0rJn"
   },
   "outputs": [],
   "source": [
    "#setup\n",
    "!pip install PyPDF2\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install nmslib\n",
    "!pip install sentence-transformers\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_hub\n",
    "!pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6ocnSoz8RWO",
   "metadata": {
    "id": "-6ocnSoz8RWO"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "import PyPDF2\n",
    "filename = 'C:\\\\Users\\\\syuhh\\\\corpus_textbook\\\\textbook_combined.pdf'\n",
    "pdf_file = open(filename, 'rb')\n",
    "ind_textbook = PyPDF2.PdfReader(pdf_file)\n",
    "total_pages = len(ind_textbook.pages)\n",
    "\n",
    "count = 0\n",
    "rawtext  = ''\n",
    "\n",
    "# loop through to read each page from the pdf file\n",
    "while(count < total_pages):\n",
    "    # Get the specified number of pages in the document\n",
    "    textbook_page  = ind_textbook.pages[count]\n",
    "    # Process the next page\n",
    "    count += 1\n",
    "    # Extract the text from the page\n",
    "    rawtext += textbook_page.extract_text()\n",
    "\n",
    "with open('C:\\\\Users\\\\syuhh\\\\corpus_textbook\\\\rawtext.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(rawtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleansing\n",
    "f = open('C:\\\\Users\\\\syuhh\\\\corpus_textbook\\\\rawtext_clean.txt', 'r', encoding=\"utf-8\")\n",
    "rawtext = f.read()\n",
    "import re\n",
    "#pattern = r\"-\\nI[A-Z][a-z]\"\n",
    "#pattern = r\"c[1-2][0-9]\\.fm.*\\n\"\n",
    "#pattern = r\"-\\n\"\n",
    "#pattern = r\"([a-z])\\n([A-Z])\"\n",
    "#pattern = r\"([a-z])+\\s*\\n([A-Z])\"\n",
    "pattern = r\"\\.\\s{1,}([a-z])\"\n",
    "replacement = r\"\\1.\\n\\2\"\n",
    "result = re.sub(pattern, replacement, rawtext)\n",
    "\n",
    "with open('C:\\\\Users\\\\syuhh\\\\corpus_textbook\\\\rawtext_clean.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e53b96c",
   "metadata": {
    "executionInfo": {
     "elapsed": 10005,
     "status": "ok",
     "timestamp": 1687878146066,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "1e53b96c"
   },
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(filename):\n",
    "    f = open(filename,'r',encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    #Lowercasing\n",
    "    text_l = text.lower()\n",
    "    # Numbers removal\n",
    "    text_ld = \"\".join([c for c in text_l if not c.isdigit()])\n",
    "    #Tokenisation + Stopwords removal + Lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    exc =[\"if\",\"for\",\"do\",\"while\",\"same\",\"all\",\"where\",\"how\",\"when\",\"what\",\"which\",\"who\"]\n",
    "    for w in exc:\n",
    "        stop_words.remove(w)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data = []\n",
    "    sentences = []\n",
    "    # iterate through each sentence in the file\n",
    "    for i in sent_tokenize(text_ld):\n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        # tokenize the sentence into words\n",
    "        for j in word_tokenize(i):\n",
    "            if j not in string.punctuation:\n",
    "                temp2.append(j)\n",
    "            #Punctuation and non-alphanumeric character removal\n",
    "                if j not in stop_words:\n",
    "                    j = lemmatizer.lemmatize(j)\n",
    "                    temp.append(j)\n",
    "        sentences.append(\" \".join(temp2))\n",
    "        data.append(temp)\n",
    "    #Named entity recognition no run\n",
    "    #Stemming no run\n",
    "    return data, sentences\n",
    "\n",
    "#data, sentences = preprocess('/content/drive/MyDrive/Colab Notebooks/rawtext.txt')\n",
    "data, sentences = preprocess(\"C:\\\\Users\\\\syuhh\\\\corpus_textbook\\\\rawtext_clean.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1aa5912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['preface'], ['textbook', 'began', 'set', 'lecture', 'note', 'for', 'first-year', 'undergraduate', 'software', 'engineering', 'course'], ['course', 'run', 'week', 'semester', 'two', 'lecture', 'week'], ['intention', 'text', 'cover', 'topic', 'c', 'programming', 'language', 'introductory', 'software', 'design', 'sequence', 'lecture', 'course', 'material', 'chapter', 'well', 'served', 'two', 'lecture', 'apiece'], ['ample', 'cross-referencing', 'indexing', 'provided', 'make', 'text', 'serviceable', 'reference', 'complete', 'work', 'recommended'], ['particular', 'for', 'practicing', 'programmer', 'best', 'available', 'tutorial', 'reference', 'kernighan', 'ritchie', 'best', 'in-depth', 'reference', 'harbison', 'steele'], ['influence', 'two', 'work', 'text', 'readily', 'apparent', 'throughout'], ['what', 'set', 'book', 'apart', 'introductory', 'c-programming', 'text', 'strong', 'emphasis', 'software', 'design'], ['like', 'text', 'present', 'core', 'language', 'syntax', 'semantics', 'also', 'address', 'aspect', 'program', 'composition', 'function', 'interface', 'section'], ['file', 'modularity', 'section']]\n",
      "39633\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd0d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preface', 'this textbook began as a set of lecture notes for a first-year undergraduate software engineering course in', 'the course was run over a week semester with two lectures a week', 'the intention of this text is to cover topics on the c programming language and introductory software design in sequence as a lecture course with the material in chapters and well served by two lectures apiece', 'ample cross-referencing and indexing is provided to make the text a serviceable reference but more complete works are recommended']\n",
      "39633\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8632c0d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113010,
     "status": "ok",
     "timestamp": 1687871219768,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "8632c0d4",
    "outputId": "e1a80ca0-f100-4725-e248-55eac3d39fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5460193\n",
      "[('do-while', 0.7650426626205444), ('statement', 0.7404494881629944), ('continue', 0.7077022194862366), ('executed', 0.681331992149353), ('break', 0.6717720627784729), ('body', 0.671444296836853), ('enclosing', 0.6347253322601318), ('transfer', 0.6344325542449951), ('while', 0.6291719675064087), ('condition', 0.6276076436042786)]\n"
     ]
    }
   ],
   "source": [
    "# word2vec cbow\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cbow_model = gensim.models.Word2Vec(data, min_count = 2, vector_size = 5000, \n",
    "                                    window = 4, hs = 1, negative=0)\n",
    "\n",
    "print(cbow_model.wv.similarity('iteration','loop'))\n",
    "similar_words1 = cbow_model.wv.most_similar('loop', topn=10)\n",
    "print(similar_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95f914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop -->\n",
      "[('statement', 0.7829611897468567), ('do-while', 0.7455766797065735), ('executed', 0.7041654586791992), ('continue', 0.6850815415382385), ('break', 0.6756772398948669), ('condition', 0.6600749492645264), ('enclosing', 0.6535517573356628), ('body', 0.6341366171836853), ('brace', 0.6253629922866821), ('while', 0.6179232597351074)]\n",
      "\n",
      " printf -->\n",
      "[('``', 0.6781397461891174), ('d/', 0.6452648639678955), ('rb', 0.6238480806350708), ('deck', 0.6188544034957886), ('\\\\n', 0.6171674728393555), ('p.on_hand', 0.612030029296875), ('world', 0.5959655046463013), ('n.kind', 0.5938659310340881), (\"''\", 0.5849100947380066), ('d\\\\n', 0.5782233476638794)]\n"
     ]
    }
   ],
   "source": [
    "print(\"loop -->\")\n",
    "similar_words1 = cbow_model.wv.most_similar('loop', topn=10)\n",
    "print(similar_words1)\n",
    "print(\"\\n printf -->\")\n",
    "similar_words2 = cbow_model.wv.most_similar('printf', topn=10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b23fb6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158870,
     "status": "ok",
     "timestamp": 1687873417976,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "3b23fb6e",
    "outputId": "6eebd5ff-4ca0-428a-9a36-ac569675113a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6550745\n",
      "[('do-while', 0.6988455653190613), ('executed', 0.6893923282623291), ('body', 0.6795340180397034), ('iteration', 0.6550743579864502), ('forever', 0.6385603547096252), ('transfer', 0.6347364783287048), ('continue', 0.632845938205719), ('statement', 0.6039971709251404), ('mistaken', 0.6009771227836609), ('enclosing', 0.5964893102645874)]\n"
     ]
    }
   ],
   "source": [
    "# word2vec skipgram\n",
    "skipgram_model = gensim.models.Word2Vec(data, min_count = 2, vector_size = 5000, \n",
    "                                        window = 4, sg = 1, hs = 1, negative=0)\n",
    "\n",
    "print(skipgram_model.wv.similarity('iteration','loop'))\n",
    "similar_words1 = skipgram_model.wv.most_similar('loop', topn=10)\n",
    "print(similar_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00d3c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop -->\n",
      "[('do-while', 0.6988455653190613), ('executed', 0.6893923282623291), ('body', 0.6795340180397034), ('iteration', 0.6550743579864502), ('forever', 0.6385603547096252), ('transfer', 0.6347364783287048), ('continue', 0.632845938205719), ('statement', 0.6039971709251404), ('mistaken', 0.6009771227836609), ('enclosing', 0.5964893102645874)]\n",
      "\n",
      " printf -->\n",
      "[('``', 0.6267349123954773), ('i/j', 0.6042519211769104), ('d\\\\n', 0.5987091064453125), ('order.amount', 0.5899890661239624), ('s.flag', 0.5711936950683594), ('quarter', 0.5668281316757202), ('nickel', 0.5653830170631409), ('km\\\\n', 0.5620003342628479), ('ld\\\\n', 0.5617414116859436), ('prime\\\\n', 0.560014009475708)]\n"
     ]
    }
   ],
   "source": [
    "print(\"loop -->\")\n",
    "similar_words1 = skipgram_model.wv.most_similar('loop', topn=10)\n",
    "print(similar_words1)\n",
    "print(\"\\n printf -->\")\n",
    "similar_words2 = skipgram_model.wv.most_similar('printf', topn=10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4803aac",
   "metadata": {
    "executionInfo": {
     "elapsed": 92458,
     "status": "ok",
     "timestamp": 1687872887707,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "a4803aac"
   },
   "outputs": [],
   "source": [
    "# bert + nmslib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "words = [w for s in data for w in s]\n",
    "df = pd.DataFrame({\"words\": words})\n",
    "word_data = list(df[\"words\"].to_list())\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "encoding = model.encode(word_data)\n",
    "\n",
    "data_encoding = np.array(encoding)\n",
    "np.save(\"encoding.npy\", data_encoding)\n",
    "embedding= np.load(\"encoding.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e6a048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1687872931022,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "74e6a048",
    "outputId": "f69f391e-d0f5-4ee2-cfae-c578633e86ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.010352628, 0.07233449, 0.024498053, 0.0570...</td>\n",
       "      <td>preface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.039159015, 0.025386218, -0.043094616, 0.07...</td>\n",
       "      <td>textbook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.012888496, -0.024884595, -0.011723027, 0.0...</td>\n",
       "      <td>began</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.017035225, 0.051864002, -0.046009693, 0.03...</td>\n",
       "      <td>set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.015049514, 0.044220474, -0.052697603, -0.0...</td>\n",
       "      <td>lecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320617</th>\n",
       "      <td>[0.08597011, 0.06067221, -0.06504023, -0.01759...</td>\n",
       "      <td>minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320618</th>\n",
       "      <td>[-0.086003326, 0.00682386, 0.004462829, 0.0483...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320619</th>\n",
       "      <td>[-0.086003326, 0.00682386, 0.004462829, 0.0483...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320620</th>\n",
       "      <td>[0.04064646, 0.043968488, -0.017024042, 0.0127...</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320621</th>\n",
       "      <td>[-0.03996311, 0.06006253, -0.055035185, 0.0137...</td>\n",
       "      <td>number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   vector       title\n",
       "0       [-0.010352628, 0.07233449, 0.024498053, 0.0570...     preface\n",
       "1       [-0.039159015, 0.025386218, -0.043094616, 0.07...    textbook\n",
       "2       [-0.012888496, -0.024884595, -0.011723027, 0.0...       began\n",
       "3       [-0.017035225, 0.051864002, -0.046009693, 0.03...         set\n",
       "4       [-0.015049514, 0.044220474, -0.052697603, -0.0...     lecture\n",
       "...                                                   ...         ...\n",
       "320617  [0.08597011, 0.06067221, -0.06504023, -0.01759...     minimum\n",
       "320618  [-0.086003326, 0.00682386, 0.004462829, 0.0483...           n\n",
       "320619  [-0.086003326, 0.00682386, 0.004462829, 0.0483...           n\n",
       "320620  [0.04064646, 0.043968488, -0.017024042, 0.0127...  normalized\n",
       "320621  [-0.03996311, 0.06006253, -0.055035185, 0.0137...      number\n",
       "\n",
       "[320622 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizing The Embedding Of Words\n",
    "embedding= np.load(\"encoding.npy\")\n",
    "\n",
    "words = [w for s in data for w in s]\n",
    "df = pd.DataFrame({\"words\": words})\n",
    "\n",
    "vector_list = []\n",
    "for emb in embedding:\n",
    "    vector_list.append(emb)\n",
    "\n",
    "vect = np.array(vector_list)\n",
    "\n",
    "df = pd.DataFrame({\"vector\":vector_list, \"title\":df[\"words\"]})\n",
    "df = df.reset_index()\n",
    "df.drop(columns=[\"index\"],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3087edeb",
   "metadata": {
    "executionInfo": {
     "elapsed": 2090059,
     "status": "ok",
     "timestamp": 1687877549509,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "3087edeb"
   },
   "outputs": [],
   "source": [
    "import nmslib\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "class NMSLIBIndex():\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.dimension = vectors.shape[0]\n",
    "        self.vectors = vectors.astype(\"float32\")\n",
    "        self.labels = labels\n",
    "\n",
    "    def build(self):\n",
    "        self.index = nmslib.init(space='cosinesimil',method='hnsw')\n",
    "        self.index.addDataPointBatch(self.vectors)\n",
    "        self.index.createIndex({\"post\":2})\n",
    "\n",
    "    def query(self,vector,k):\n",
    "        indices = self.index.knnQuery(vector, k=k)\n",
    "        return [self.labels[i] for i in indices[0]],[round(i,2) for i in indices[1]]\n",
    "\n",
    "# Building The Search Index\n",
    "nm_index = NMSLIBIndex(vect,df[\"title\"])\n",
    "nm_index.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9dc98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting The Nearest Neighbors For Each Word (most similar words)\n",
    "# looking for n/K NNs for each word\n",
    "K = 10\n",
    "\n",
    "pd_dict= {\"main_word\":[], \"similar_word\":[], \"distance\":[], \"group\":[]}\n",
    "group = 0\n",
    "\n",
    "for vector,label in zip(df['vector'], df['title']):\n",
    "    queries_nm, distances_nm = nm_index.query(vector,K)\n",
    "    for q,d in zip(queries_nm, distances_nm):\n",
    "            if d>0.1:\n",
    "                pd_dict[\"main_word\"].append(label)\n",
    "                pd_dict[\"similar_word\"].append(q)\n",
    "                pd_dict[\"distance\"].append(d)\n",
    "                pd_dict[\"group\"].append(group)\n",
    "\n",
    "    group += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "EzXXhOQ0Io5-",
   "metadata": {
    "executionInfo": {
     "elapsed": 8593,
     "status": "ok",
     "timestamp": 1687877926616,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "EzXXhOQ0Io5-"
   },
   "outputs": [],
   "source": [
    "# Summarizing The Results\n",
    "\n",
    "frame = pd.DataFrame(pd_dict)\n",
    "frame.drop_duplicates(subset=\"similar_word\",inplace=True)\n",
    "\n",
    "syns = []\n",
    "import collections\n",
    "t = collections.defaultdict(list)\n",
    "\n",
    "for g in set(frame[\"group\"].to_list()):\n",
    "    rows = frame[frame[\"group\"] == g][\"similar_word\"].to_list()\n",
    "    rows2 = frame[frame[\"group\"] == g][\"main_word\"].to_list()[0]\n",
    "    t[rows2] = rows\n",
    "    rows.append(rows2)\n",
    "    syns.append(rows)\n",
    "    \n",
    "# similar word for a given input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "807c0ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_word</th>\n",
       "      <th>similar_word</th>\n",
       "      <th>distance</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>preface</td>\n",
       "      <td>introduction</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textbook</td>\n",
       "      <td>need</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>began</td>\n",
       "      <td>attempted</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>began</td>\n",
       "      <td>attempting</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lecture</td>\n",
       "      <td>instructor</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lecture</td>\n",
       "      <td>teaching</td>\n",
       "      <td>0.37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>lecture</td>\n",
       "      <td>class</td>\n",
       "      <td>0.37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>note</td>\n",
       "      <td>’</td>\n",
       "      <td>0.56</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>first-year</td>\n",
       "      <td>beginning</td>\n",
       "      <td>0.40</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>undergraduate</td>\n",
       "      <td>college</td>\n",
       "      <td>0.27</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>undergraduate</td>\n",
       "      <td>semester</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>software</td>\n",
       "      <td>program</td>\n",
       "      <td>0.28</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>engineering</td>\n",
       "      <td>engineer</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>week</td>\n",
       "      <td>since</td>\n",
       "      <td>0.53</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>semester</td>\n",
       "      <td>campus</td>\n",
       "      <td>0.30</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>semester</td>\n",
       "      <td>undergraduate</td>\n",
       "      <td>0.32</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>intention</td>\n",
       "      <td>behavior</td>\n",
       "      <td>0.57</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>cover</td>\n",
       "      <td>name</td>\n",
       "      <td>0.55</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>material</td>\n",
       "      <td>making</td>\n",
       "      <td>0.52</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>served</td>\n",
       "      <td>serve</td>\n",
       "      <td>0.21</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>apiece</td>\n",
       "      <td>specified</td>\n",
       "      <td>0.55</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>ample</td>\n",
       "      <td>c</td>\n",
       "      <td>0.67</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>cross-referencing</td>\n",
       "      <td>cross-referencer</td>\n",
       "      <td>0.10</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>cross-referencing</td>\n",
       "      <td>self-referencing</td>\n",
       "      <td>0.26</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>cross-referencing</td>\n",
       "      <td>referencing</td>\n",
       "      <td>0.26</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>cross-referencing</td>\n",
       "      <td>non-self-referencing</td>\n",
       "      <td>0.34</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>cross-referencing</td>\n",
       "      <td>pass-by-reference</td>\n",
       "      <td>0.35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>indexing</td>\n",
       "      <td>loop</td>\n",
       "      <td>0.69</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>serviceable</td>\n",
       "      <td>complete</td>\n",
       "      <td>0.66</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>recommended</td>\n",
       "      <td>use</td>\n",
       "      <td>0.48</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>practicing</td>\n",
       "      <td>practice</td>\n",
       "      <td>0.13</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>best</td>\n",
       "      <td>last</td>\n",
       "      <td>0.50</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>available</td>\n",
       "      <td>want</td>\n",
       "      <td>0.47</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>tutorial</td>\n",
       "      <td>code</td>\n",
       "      <td>0.53</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>kernighan</td>\n",
       "      <td>beer</td>\n",
       "      <td>0.70</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>in-depth</td>\n",
       "      <td>internal</td>\n",
       "      <td>0.51</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>harbison</td>\n",
       "      <td>sthelady</td>\n",
       "      <td>0.53</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>harbison</td>\n",
       "      <td>farbeyond</td>\n",
       "      <td>0.54</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>harbison</td>\n",
       "      <td>sthere</td>\n",
       "      <td>0.55</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>harbison</td>\n",
       "      <td>barnsworth</td>\n",
       "      <td>0.56</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>steele</td>\n",
       "      <td>int</td>\n",
       "      <td>0.67</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>influence</td>\n",
       "      <td>effect</td>\n",
       "      <td>0.34</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>readily</td>\n",
       "      <td>rapidly</td>\n",
       "      <td>0.41</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>apparent</td>\n",
       "      <td>thus</td>\n",
       "      <td>0.51</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>throughout</td>\n",
       "      <td>for</td>\n",
       "      <td>0.58</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>book</td>\n",
       "      <td>character</td>\n",
       "      <td>0.50</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>apart</td>\n",
       "      <td>compare</td>\n",
       "      <td>0.52</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>c-programming</td>\n",
       "      <td>compiler</td>\n",
       "      <td>0.35</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>strong</td>\n",
       "      <td>support</td>\n",
       "      <td>0.52</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>emphasis</td>\n",
       "      <td>``</td>\n",
       "      <td>0.55</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             main_word          similar_word  distance  group\n",
       "0              preface          introduction      0.33      0\n",
       "7             textbook                  need      0.67      1\n",
       "17               began             attempted      0.53      2\n",
       "18               began            attempting      0.55      2\n",
       "27             lecture            instructor      0.36      4\n",
       "29             lecture              teaching      0.37      4\n",
       "30             lecture                 class      0.37      4\n",
       "33                note                     ’      0.56      5\n",
       "43          first-year             beginning      0.40      7\n",
       "52       undergraduate               college      0.27      8\n",
       "59       undergraduate              semester      0.32      8\n",
       "60            software               program      0.28      9\n",
       "70         engineering              engineer      0.11     10\n",
       "72                week                 since      0.53     14\n",
       "82            semester                campus      0.30     15\n",
       "90            semester         undergraduate      0.32     15\n",
       "107          intention              behavior      0.57     19\n",
       "117              cover                  name      0.55     21\n",
       "163           material                making      0.52     32\n",
       "183             served                 serve      0.21     35\n",
       "195             apiece             specified      0.55     38\n",
       "205              ample                     c      0.67     39\n",
       "215  cross-referencing      cross-referencer      0.10     40\n",
       "216  cross-referencing      self-referencing      0.26     40\n",
       "217  cross-referencing           referencing      0.26     40\n",
       "218  cross-referencing  non-self-referencing      0.34     40\n",
       "219  cross-referencing     pass-by-reference      0.35     40\n",
       "222           indexing                  loop      0.69     41\n",
       "232        serviceable              complete      0.66     45\n",
       "242        recommended                   use      0.48     49\n",
       "252         practicing              practice      0.13     52\n",
       "272               best                  last      0.50     54\n",
       "282          available                  want      0.47     55\n",
       "292           tutorial                  code      0.53     56\n",
       "302          kernighan                  beer      0.70     58\n",
       "322           in-depth              internal      0.51     61\n",
       "332           harbison              sthelady      0.53     63\n",
       "333           harbison             farbeyond      0.54     63\n",
       "334           harbison                sthere      0.55     63\n",
       "335           harbison            barnsworth      0.56     63\n",
       "339             steele                   int      0.67     64\n",
       "349          influence                effect      0.34     65\n",
       "359            readily               rapidly      0.41     69\n",
       "362           apparent                  thus      0.51     70\n",
       "372         throughout                   for      0.58     71\n",
       "382               book             character      0.50     74\n",
       "392              apart               compare      0.52     75\n",
       "412      c-programming              compiler      0.35     77\n",
       "422             strong               support      0.52     79\n",
       "432           emphasis                    ``      0.55     80"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4e00abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " loop -->\n",
      "['loop', 'indexing']\n",
      "\n",
      " printf -->\n",
      "['printf', 'printing']\n",
      "['char', 'printf']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n loop -->\")\n",
    "for group in syns:\n",
    "    for word in group:\n",
    "        if word ==\"loop\":\n",
    "              print(group)\n",
    "\n",
    "print(\"\\n printf -->\")\n",
    "for group in syns:\n",
    "    for word in group:\n",
    "        if word ==\"printf\":\n",
    "              print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e95843c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858322,
     "status": "ok",
     "timestamp": 1687879057540,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "8e95843c",
    "outputId": "5874354e-9c85-4585-f115-408af00a6c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3045722\n",
      "[('alteration', 0.8559949994087219), ('ration', 0.7898642420768738), ('iteration-statement', 0.770746111869812), ('zation', 0.7097257971763611), ('consideration', 0.6981011033058167), ('ation', 0.6961163878440857), ('operation', 0.6848737597465515), ('generation', 0.6745608448982239), ('duration', 0.6555023193359375), ('configuration', 0.6542657017707825)]\n"
     ]
    }
   ],
   "source": [
    "# fasttext\n",
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(vector_size=2000, window=4, min_count=2)  # instantiate\n",
    "fasttext_model .build_vocab(data)\n",
    "fasttext_model .train(data, total_examples=len(data), epochs=100)  # train\n",
    "\n",
    "print(fasttext_model.wv.similarity('iteration','loop'))\n",
    "similar_words1 = fasttext_model.wv.most_similar(positive=['iteration'], topn = 10)\n",
    "print(similar_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc8562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop -->\n",
      "[('timeloop', 0.7359610199928284), ('loop_done', 0.729006826877594), ('looping', 0.6567798852920532), ('loose', 0.3977554440498352), ('ifstatement', 0.38792967796325684), ('substatement', 0.38574081659317017), ('do-while', 0.3837915062904358), ('op', 0.37323707342147827), ('statement', 0.3693470060825348), ('iteration-statement', 0.36146387457847595)]\n",
      "\n",
      " printf -->\n",
      "[('vprintf', 0.8317158818244934), ('v…printf', 0.8176165223121643), ('wprintf', 0.8175871968269348), ('ofprintf', 0.8169824481010437), ('…printf', 0.8120958805084229), ('vwprintf', 0.8064144849777222), ('rintf', 0.7987238168716431), ('sprintf', 0.790643036365509), ('vsprintf', 0.786302387714386), ('fwprintf', 0.7817254066467285)]\n"
     ]
    }
   ],
   "source": [
    "print(\"loop -->\")\n",
    "similar_words1 = fasttext_model.wv.most_similar(positive=['loop'], topn = 10)\n",
    "print(similar_words1)\n",
    "print(\"\\n printf -->\")\n",
    "similar_words2 = fasttext_model.wv.most_similar(positive=['printf'], topn = 10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de93fac5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1687879269221,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "de93fac5",
    "outputId": "3e122bfe-c4b9-4074-817c-12e7eeaf7832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# pretrained model library\n",
    "\n",
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c1580",
   "metadata": {},
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec_pretrained = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "print(word2vec_pretrained.similarity('iteration','loop'))\n",
    "similar_words1 = word2vec_pretrained.most_similar(positive=['iteration'], topn = 10)\n",
    "print(similar_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c4ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller size of Word2Vec pretrained\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "word2vec_smaller = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "word2vec_pretrained = gensim.models.KeyedVectors.load_word2vec_format(word2vec_smaller, binary=False)\n",
    "\n",
    "#print(word2vec_pretrained.similarity('iteration','loop'))\n",
    "#similar_words1 = word2vec_pretrained.most_similar(positive=['iteration'], topn = 10)\n",
    "#print(similar_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fb5f02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "incompatible vector size 300 in file C:\\Users\\syuhh\\AppData\\Roaming\\nltk_data\\models\\word2vec_sample\\pruned.word2vec.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e98f19969195>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# then it is left as is in the original model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mword2vec_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mword2vec_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_pretrained_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlockf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mword2vec_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword2vec_tuned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mintersect_word2vec_format\u001b[1;34m(self, fname, lockf, binary, encoding, unicode_errors)\u001b[0m\n\u001b[0;32m   1751\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"incompatible vector size %d in file %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m                 \u001b[1;31m# TODO: maybe mismatched vectors still useful enough to merge (truncating/padding)?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: incompatible vector size 300 in file C:\\Users\\syuhh\\AppData\\Roaming\\nltk_data\\models\\word2vec_sample\\pruned.word2vec.txt"
     ]
    }
   ],
   "source": [
    "#pretrained Word2Vec fine-tuning\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_tuned = Word2Vec(vector_size=100, min_count=2)\n",
    "word2vec_tuned.build_vocab(data)\n",
    "\n",
    "#word2vec_pretrained_path = \"C:\\\\Users\\\\syuhh\\\\gensim-data\\\\word2vec-google-news-300\n",
    "#\\\\word2vec-google-news-300.gz\"\n",
    "#unable to run above as the resoures needed is too large\n",
    "# a pruned version is used (vocab contains 43981 words)\n",
    "word2vec_pretrained_path = \"C:\\\\Users\\\\syuhh\\\\AppData\\\\Roaming\\\\nltk_data\\\\models\\\\word2vec_sample\\\\pruned.word2vec.txt\"\n",
    "word2vec_pretrained = KeyedVectors.load_word2vec_format(word2vec_pretrained_path, binary=False)\n",
    "\n",
    "# Add the pre-trained model vocabulary\n",
    "import numpy as np\n",
    "word2vec_tuned.build_vocab([list(word2vec_pretrained.index_to_key)], update=True)\n",
    "total_examples = word2vec_tuned.corpus_count\n",
    "\n",
    "# Load the pre-trained models embeddings\n",
    "# note: if a word doesn't exist in the pre-trained vocabulary \n",
    "# then it is left as is in the original model\n",
    "word2vec_tuned.wv.vectors_lockf = np.ones(len(word2vec_tuned.wv), dtype=np.float16)\n",
    "word2vec_tuned.wv.intersect_word2vec_format(word2vec_pretrained_path, binary=False, lockf=1.0)\n",
    "\n",
    "word2vec_tuned.train(data, total_examples=total_examples, epochs=word2vec_tuned.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e34493",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n loop -->\")\n",
    "similar_words1 = word2vec_tuned.wv.most_similar(positive=['loop'], topn = 10)\n",
    "print(similar_words1)\n",
    "print(\"\\n printf -->\")\n",
    "similar_words2 = word2vec_tuned.wv.most_similar(positive=['printf'], topn = 10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdf66dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172262,
     "status": "ok",
     "timestamp": 1687879499302,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "76ab854c",
    "outputId": "d41c05d8-4608-4bda-88a0-1f2740f42317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "0.3161096\n",
      "[('iterations', 0.645356297492981), ('algorithm', 0.5090993642807007), ('iterative', 0.4905813932418823), ('computed', 0.4452853500843048), ('approximation', 0.429524302482605), ('computation', 0.4205939769744873), ('codice_1', 0.4104224443435669), ('instalment', 0.405374139547348), ('permutation', 0.4050779342651367), ('computes', 0.40470555424690247)]\n"
     ]
    }
   ],
   "source": [
    "#pretrained GloVe model\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#run below for first run\n",
    "glove_pretrained = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "glove_pretrained_path = \"C:\\\\Users\\\\syuhh\\\\gensim-data\\\\glove-wiki-gigaword-300\\\\glove-wiki-gigaword-300.gz\"\n",
    "glove_pretrained = KeyedVectors.load_word2vec_format(glove_pretrained_path, binary=False)\n",
    "\n",
    "print(glove_pretrained.similarity('iteration','loop'))\n",
    "similar_words1 = glove_pretrained.most_similar(positive=['iteration'], topn = 10)\n",
    "print(similar_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020be038",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 85925,
     "status": "error",
     "timestamp": 1687881684982,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "020be038",
    "outputId": "4b1ec8d6-b17c-4c24-d356-496d4cf71702"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:EPOCH 0: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 1: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 2: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 3: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 4: supplied example count (39633) did not equal expected count (1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1351625, 1603110)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pretrained GloVe model fine-tuning\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_tuned = Word2Vec(vector_size=300, min_count=2)\n",
    "glove_tuned.build_vocab(data)\n",
    "\n",
    "glove_pretrained_path = \"C:\\\\Users\\\\syuhh\\\\gensim-data\\\\glove-wiki-gigaword-300\\\\glove-wiki-gigaword-300.gz\"\n",
    "glove_pretrained = KeyedVectors.load_word2vec_format(glove_pretrained_path, binary=False)\n",
    "\n",
    "# Add the pre-trained model vocabulary\n",
    "import numpy as np\n",
    "glove_tuned.build_vocab([list(glove_pretrained.index_to_key)], update=True)\n",
    "total_examples = glove_tuned.corpus_count\n",
    "\n",
    "# Load the pre-trained models embeddings\n",
    "# note: if a word doesn't exist in the pre-trained vocabulary \n",
    "# then it is left as is in the original model\n",
    "glove_tuned.wv.vectors_lockf = np.ones(len(glove_tuned.wv), dtype=np.float32)\n",
    "glove_tuned.wv.intersect_word2vec_format(glove_pretrained_path, binary=False, lockf=1.0)\n",
    "\n",
    "glove_tuned.train(data, total_examples=total_examples, epochs=glove_tuned.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "648f21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " loop -->\n",
      "[('intx', 0.7398110032081604), ('x++', 0.7298818230628967), ('do-while', 0.7262066602706909), ('x=', 0.7172942757606506), ('z=', 0.7151269912719727), ('expr', 0.711052656173706), ('y=', 0.7076507210731506), ('++i', 0.6921600699424744), ('=t', 0.6815797090530396), ('c=getchar', 0.6805795431137085)]\n",
      "\n",
      " printf -->\n",
      "[('type\\\\n', 0.438312828540802), ('variadic', 0.39523354172706604), ('ld\\\\n', 0.3732050657272339), ('—which', 0.3637981414794922), ('s\\\\n', 0.3592255413532257), ('denom', 0.35905635356903076), ('d\\\\n', 0.3566039204597473), ('function', 0.355427622795105), ('.s\\\\n', 0.3521178364753723), ('f\\\\n', 0.34731367230415344)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n loop -->\")\n",
    "similar_words1 = glove_tuned.wv.most_similar(positive=['loop'], topn = 10)\n",
    "print(similar_words1)\n",
    "print(\"\\n printf -->\")\n",
    "similar_words2 = glove_tuned.wv.most_similar(positive=['printf'], topn = 10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d88ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
      "0.5485749\n",
      "[('intx', 0.7398110032081604), ('x++', 0.7298818230628967), ('do-while', 0.7262066602706909), ('x=', 0.7172942757606506), ('z=', 0.7151269912719727), ('expr', 0.711052656173706), ('y=', 0.7076507210731506), ('++i', 0.6921600699424744), ('=t', 0.6815797090530396), ('c=getchar', 0.6805795431137085)]\n",
      "[('fprintf', 0.8410983085632324), ('snprintf', 0.8028349876403809), ('sprintf', 0.7848883271217346), ('scanf', 0.7585453987121582), ('println', 0.7295418977737427), ('stdio.h', 0.709204912185669), ('memcpy', 0.6922029256820679), ('stdio', 0.6906760334968567), ('strcpy', 0.6871660947799683), ('strncpy', 0.6685115098953247)]\n"
     ]
    }
   ],
   "source": [
    "#pretrained fasttext\n",
    "#pretrained GloVe model\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# run below for first run\n",
    "#fasttext_pretrained = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "fasttext_pretrained_path = \"C:\\\\Users\\\\syuhh\\\\gensim-data\\\\fasttext-wiki-news-subwords-300\\\\fasttext-wiki-news-subwords-300.gz\"\n",
    "fasttext_pretrained = KeyedVectors.load_word2vec_format(fasttext_pretrained_path, binary=False)\n",
    "\n",
    "print(fasttext_pretrained.similarity('iteration','loop'))\n",
    "similar_words = fasttext_pretrained.most_similar(positive=['iteration'], topn = 10)\n",
    "print(similar_words1)\n",
    "similar_words2 = fasttext_pretrained.most_similar(positive=['printf'], topn = 10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9624db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:EPOCH 0: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 1: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 2: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 3: supplied example count (39633) did not equal expected count (1)\n",
      "WARNING:gensim.models.word2vec:EPOCH 4: supplied example count (39633) did not equal expected count (1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1352269, 1603110)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pretrained fasttext model fine-tuning\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fasttext_tuned = Word2Vec(vector_size=300, min_count=2)\n",
    "fasttext_tuned.build_vocab(data)\n",
    "\n",
    "fasttext_pretrained_path = \"\"\"C:\\\\Users\\\\syuhh\\\\gensim-data\\\\\n",
    "fasttext-wiki-news-subwords-300\\\\fasttext-wiki-news-subwords-300.gz\"\"\"\n",
    "fasttext_pretrained = KeyedVectors.load_word2vec_format(fasttext_pretrained_path, binary=False)\n",
    "\n",
    "# Add the pre-trained model vocabulary\n",
    "import numpy as np\n",
    "fasttext_tuned.build_vocab([list(fasttext_pretrained.index_to_key)], update=True)\n",
    "total_examples = fasttext_tuned.corpus_count\n",
    "\n",
    "# Load the pre-trained models embeddings\n",
    "# note: if a word doesn't exist in the pre-trained vocabulary \n",
    "# then it is left as is in the original model\n",
    "fasttext_tuned.wv.vectors_lockf = np.ones(len(fasttext_tuned.wv), dtype=np.float32)\n",
    "fasttext_tuned.wv.intersect_word2vec_format(fasttext_pretrained_path, binary=False, lockf=1.0)\n",
    "\n",
    "fasttext_tuned.train(data, total_examples=total_examples, epochs=fasttext_tuned.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa51abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop -->\n",
      "[('x=', 0.9718918800354004), ('y=', 0.9707276225090027), ('z=', 0.9702528119087219), ('caselabel', 0.9666198492050171), ('expression', 0.9656054377555847), ('intn', 0.9637839794158936), ('statement', 0.9631353616714478), ('name-hiding', 0.9630696773529053), ('while', 0.9628068208694458), ('•functions', 0.9626761674880981)]\n",
      "\n",
      " printf -->\n",
      "[(\"''\", 0.9262217283248901), ('\\\\n', 0.925439178943634), ('d\\\\n', 0.9240949153900146), ('``', 0.9222123622894287), ('d\\\\t', 0.9096987247467041), ('world\\\\n', 0.9088935852050781), ('.f\\\\n', 0.9069238901138306), ('i++', 0.9065861105918884), ('e\\\\t', 0.9050455093383789), ('\\\\t', 0.9048459529876709)]\n"
     ]
    }
   ],
   "source": [
    "print(\"loop -->\")\n",
    "similar_words1 = fasttext_tuned.wv.most_similar(positive=['loop'], topn = 10)\n",
    "print(similar_words1)\n",
    "print(\"\\n printf -->\")\n",
    "similar_words2 = fasttext_tuned.wv.most_similar(positive=['printf'], topn = 10)\n",
    "print(similar_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b5aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.save(\"word_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bae23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32893,
     "status": "ok",
     "timestamp": 1687882616803,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "409bae23",
    "outputId": "34e6befd-a5b0-4d9e-a3ca-3e958660e9e2"
   },
   "outputs": [],
   "source": [
    "#sentence level embedding\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model2 = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "data_encoding = np.array(embeddings)\n",
    "np.save(\"sents_embedding.npy\", data_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cab55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "error",
     "timestamp": 1687884966115,
     "user": {
      "displayName": "Yuh Heng Sin",
      "userId": "00228757380582750778"
     },
     "user_tz": -480
    },
    "id": "360cab55",
    "outputId": "0a71235a-9ffd-41cf-a0e2-685fca2a1c6b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "embeddings = np.load(\"sents_embedding.npy\")\n",
    "\n",
    "user_query = ['what is loop in C']\n",
    "query_embedding = model.encode(user_query)\n",
    "\n",
    "sim = np.zeros((len(user_query), len(sentences)))\n",
    "\n",
    "for i in range(len(user_query)):\n",
    "    for j in range(len(sentences)):\n",
    "        sim[i,j] = cos_sim(query_embedding[i], embeddings[j])\n",
    "\n",
    "most_related = sorted(zip(sim[0], sentences), reverse=True)[:3]\n",
    "print(most_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788703e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"sents_embedding.npy\")\n",
    "\n",
    "user_query = ['statements that executes for a specified number of repetitions']\n",
    "query_embedding = model.encode(user_query)\n",
    "\n",
    "sim = np.zeros((len(user_query), len(sentences)))\n",
    "\n",
    "for i in range(len(user_query)):\n",
    "    for j in range(len(sentences)):\n",
    "        sim[i,j] = cos_sim(query_embedding[i], embeddings[j])\n",
    "\n",
    "most_related = sorted(zip(sim[0], sentences), reverse=True)[:3]\n",
    "print(most_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"sents_embedding.npy\")\n",
    "\n",
    "user_query = ['loop']\n",
    "query_embedding = model.encode(user_query)\n",
    "\n",
    "sim = np.zeros((len(user_query), len(sentences)))\n",
    "\n",
    "for i in range(len(user_query)):\n",
    "    for j in range(len(sentences)):\n",
    "        sim[i,j] = cos_sim(query_embedding[i], embeddings[j])\n",
    "\n",
    "most_related = sorted(zip(sim[0], sentences), reverse=True)[:3]\n",
    "print(most_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4725c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loops while and fr\n",
      "exit from loop\n",
      "do-while loops\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ELMo\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "filtered = [sen for sen in sentences if len(sen.split()) > 1 and len(sen.split()) < 7]\n",
    "# Load ELMo model\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/2\")\n",
    "\n",
    "# Generate embeddings for example sentences\n",
    "elmo_embeddings = elmo.signatures[\"default\"](tf.constant(filtered))[\"default\"]\n",
    "elmo_vect = elmo_embeddings.numpy()\n",
    "\n",
    "# Search string\n",
    "search_string = [\"loop\"]\n",
    "# Generate embeddings for search string\n",
    "elmo_embeddings2 = elmo.signatures[\"default\"](tf.constant(search_string))[\"default\"]\n",
    "search_vect = elmo_embeddings2.numpy()\n",
    "\n",
    "# Reshape vectors for cosine similarity calculation\n",
    "search_vect = search_vect.reshape(1, -1)\n",
    "elmo_vect = elmo_vect.reshape(len(filtered), -1)\n",
    "\n",
    "# Calculate cosine similarities\n",
    "cosine_similarities = np.dot(search_vect, elmo_vect.T) / (\n",
    "    np.linalg.norm(search_vect) * np.linalg.norm(elmo_vect, axis=1))\n",
    "\n",
    "output = \"\"\n",
    "for i in cosine_similarities.argsort()[0][-3:]:\n",
    "    output += filtered[i] + \"\\n\"\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269c9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sentences.data', 'wb') as fp:\n",
    "        pickle.dump(sentences, fp)\n",
    "with open('sents_inELMo.data', 'wb') as fp:\n",
    "        pickle.dump(filtered, fp)\n",
    "with open('elmo_vect.data', 'wb') as fp:\n",
    "        pickle.dump(elmo_vect, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d123bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessorc program\n",
      "program termination\n",
      "program quicksort\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search string\n",
    "search_string = [\"program\"]\n",
    "# Generate embeddings for search string\n",
    "elmo_embeddings2 = elmo.signatures[\"default\"](tf.constant(search_string))[\"default\"]\n",
    "search_vect = elmo_embeddings2.numpy()\n",
    "\n",
    "# Reshape vectors for cosine similarity calculation\n",
    "search_vect = search_vect.reshape(1, -1)\n",
    "elmo_vect = elmo_vect.reshape(len(filtered), -1)\n",
    "\n",
    "# Calculate cosine similarities\n",
    "cosine_similarities = np.dot(search_vect, elmo_vect.T) / (\n",
    "    np.linalg.norm(search_vect) * np.linalg.norm(elmo_vect, axis=1))\n",
    "\n",
    "output = \"\"\n",
    "for i in cosine_similarities.argsort()[0][-3:]:\n",
    "    output += filtered[i] + \"\\n\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0f2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
